<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>NAVI Dataset</title>
<link href="./navidataset_files/style.css" rel="stylesheet">
<script type="text/javascript" src="./navidataset_files/jquery.mlens-1.0.min.js"></script>
<script type="text/javascript" src="./navidataset_files/jquery.js"></script>
</head>

<body>
<div class="content">
  <h1><strong>NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations</strong></h1>
  <p id="authors"><span>
    <!-- Varun Jampani*, Kevis-Kokitsi Maninis*, Andreas Engelhardt, Karen Truong, Arjun Karpur, Kyle Sargent, Stefan Popov, Andre Araujo, Ricardo Martin-Brualla, Kaushal Patel, Daniel Vlasic, Vittorio Ferrari, Ameesh Makadia, Ce Liu, Yuanzhen Li, Howard Zhou (*equal contribution) -->
      <a href="https://varunjampani.github.io/">Varun Jampani*</a>
      <a href="https://www.kmaninis.com/">Kevis-Kokitsi Maninis*</a>
      <a href="https://www.linkedin.com/in/andreas-engelhardt-5a1451ab?originalSubdomain=de/">Andreas Engelhardt</a>
      <a href="https://navidataset.github.io/">Karen Truong</a>
      <a href="https://scholar.google.com/citations?user=jgSItF4AAAAJ&hl=en&oi=ao/">Arjun Karpur</a>
      <a href="https://kylesargent.github.io//">Kyle Sargent</a>
      <a href="https://www.popov.im//">Stefan Popov</a>
      <a href="https://andrefaraujo.github.io//">Andre Araujo</a>
      <a href="https://ricardomartinbrualla.com//">Ricardo Martin-Brualla</a>
      <a href="https://scholar.google.com/citations?user=US0_UBgAAAAJ&hl=en">Kaushal Patel</a>
      <a href="https://scholar.google.com/citations?user=T701RxYAAAAJ&hl=en/">Daniel Vlasic</a>
      <a href="https://sites.google.com/corp/view/vittoferrari/">Vittorio Ferrari</a>
      <a href="https://www.ameeshmakadia.com/">Ameesh Makadia</a>
      <a href="http://people.csail.mit.edu/celiu/">Ce Liu</a>
      <a href="https://people.csail.mit.edu/yzli/">Yuanzhen Li</a>
      <a href="https://scholar.google.com/citations?user=Rh9T3EcAAAAJ&hl=en/">Howard Zhou</a> <br>
      (*equal contribution)
    <br><br>
  <span style="font-size: 24px">Google
  </span></p>
  <br>
  <img src="./navidataset_files/NAVI_teaser_1.png" class="teaser-gif" style="width:100%;"><center> <h4>NAVI dataset consists of both in-the-wild and multi-view image collecitons with high-quality aligned 3D shape ground-truths</h4></center><br>
    <font size="+2">
          <p style="text-align: center;">
            <a href="" target="_blank">[Paper]</a> &nbsp;&nbsp;&nbsp;&nbsp;
	          <a href="https://github.com/google/navi" target="_blank">[Dataset]</a> &nbsp;&nbsp;&nbsp;&nbsp;
            <a href="navidataset_files/bibtex.txt" target="_blank">[BibTeX]</a>
          </p>
    </font>
</div>
<div class="content">
  <h2 style="text-align:center;">Abstract</h2>
  <p></p>
</div>

<div class="content">
  <h2>Dataset Highlights</h2>
  NAVI dataset consists of casually captured image collections with high-quality 3D shape and pose annotations. The dataset consists of multi-view and in-the-wild image collections of 36 objects with around 10K images in total. Here are some key aspects of the dataset:
    <ul style="list-style-type: circle;">
      <li><strong>In-the-wild</strong>. In addition to typical multi-view object images, NAVI provides in-the-wild images collections where objects are captured under varying backgrounds, illuminations and cameras.</li>
      <li><strong>Category-agnostic</strong>. Objects in the NAVI dataset are category-agnostic with image collections of toys and decoration items that do not have any category-specific shapes.</li>
      <li><strong>Near-perfect 3D geometry</strong>. We use high-quality 3D scanners to get 3D shape ground-truth.</li>
      <li><strong>Near-perfect camera poses</strong>. We obtain high-quality 3D camera pose annotations with manual 2D-3D alignment along with rigorous verification.</li>
      <li><strong>Derivative annotations such as dense correspondences, depth etc</strong>. Given the near-perfect 3D shape and camera parameters, one could easily derive other high-quality annotations such dense pixel level correspondences, monocular depth, foreground segmentation etc.</li>
    </ul>
  <p>
  </p>
  <br>
  <img class="summary-img" src="./navidataset_files/NAVI_samples.png" style="width:100%;"> <center> <h4>Sample NAVI dataset images and the corresponding 3D shape alignments</h4></center><br>


<div class="content">
  <h2>Wild Image Collections</h2>
  Wild image collections such as image search results or product catelogue photos are readily available in the internet and does not require any active capture efforts. To advance research on 3D shape and pose estimation from such in-the-wild online image collections, we provide image collections where the objects are captured under unique backgrounds, illuminations and camera settings.
  <p> </p>
  <br>
  <img class="summary-img" src="./navidataset_files/NAVI_wild_samples.png" style="width:100%;"><center> <h4>Sample in-the-wild image collections in the NAVI dataset with varying backgrounds, illuminations and cameras</h4> <br>
</div>

<div class="content">
  <h2>Multi-View Image Collections</h2>
  <p> </p>
  <br>
  <img class="summary-img" src="./navidataset_files/NAVI_multi_view_samples.png" style="width:100%;"> <center> <h4>Sample multi-view image collections in the NAVI dataset which are captured with hand-held cameras in natural settings</h4> <br>
</div>

<div class="content">
  <h2>Dense Pixel Correspondences</h2>
  Given the high-quality shape annotations, we can compute dense per-pixel correspondences across different object images.
  <p> </p>
  <br>
  <img class="summary-img" src="./navidataset_files/NAVI_samples.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>Monocular Depth and Foreground Segmentations</h2>
  <p> </p>
  <br>
  <img class="summary-img" src="./navidataset_files/NAVI_samples.png" style="width:100%;"> <br>
</div>

<div class="content">
  <h2>BibTex</h2>
  If you find this dataset useful, please consider citing our work:
  <code> @article{jampani2023navi,<br>
  &nbsp;&nbsp;title={NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations},<br>
  &nbsp;&nbsp;author={Jampani, Varun and Maninis, Kevis-Kokitsi and Engelhardt, Andreas and Truong, Karen and Karpur, Arjun and Sargent, Kyle and Popov, Stefan and Araujo, Andre and Martin-Brualla, Ricardo and Patel, Kaushal and Vlasic, Daniel and Ferrari, Vittorio and Makadia, Ameesh and Liu, Ce and Li, Yuanzhen and Zhou, Howard},<br>
  &nbsp;&nbsp;booktitle={arXiv preprint},<br>
  &nbsp;&nbsp;url={https://navidataset.github.io/},<br>
  &nbsp;&nbsp;year={2023}<br>
  } </code>
</div>

<!-- <div class="content" id="acknowledgements">
  <p><strong>Acknowledgements</strong>:
    We thank Rinon Gal, Adi Zicher, Ron Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or for their valuable inputs that helped improve this work, and to Mohammad Norouzi, Chitwan Saharia and William Chan for providing us with their support and the pretrained models of Imagen. Finally, a special thank you to David Salesin for his feedback, advice and for his support for the project.
    <!-- Recycling a familiar <a href="https://chail.github.io/latent-composition/">template</a> ;). -->
  </p> -->
</div>
</body>
</html>
